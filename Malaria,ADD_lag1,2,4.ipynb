{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89685298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from minisom import MiniSom\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c46e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from minisom import MiniSom\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import itertools\n",
    "\n",
    "class SOMAnalyzer:\n",
    "    def __init__(self, file1, file2):\n",
    "        self.file_path = file1\n",
    "        self.file_path_Pune = file2\n",
    "        self.cluster_infos = {}\n",
    "        self.cluster_values_dfs = {}\n",
    "\n",
    "    def read_files(self):\n",
    "        df_nagpur = pd.read_csv(self.file_path)\n",
    "        df_nagpur.replace([-999, -1], np.nan, inplace=True)\n",
    "        df_pune = pd.read_csv(self.file_path_Pune)\n",
    "        df_pune.replace([-999, -1], np.nan, inplace=True)\n",
    "        df_pune = df_pune.dropna(subset=['ADD', 'MALARIA']).reset_index(drop=True)\n",
    "        df_nagpur = df_nagpur.dropna(subset=['ADD', 'MALARIA']).reset_index(drop=True)\n",
    "        return df_nagpur, df_pune\n",
    "\n",
    "    def pop_mean(self, df):\n",
    "        df['ADD'] = df['ADD'] / (df['POPULATION'] / 100000)\n",
    "        df['MALARIA'] = df['MALARIA'] / (df['POPULATION'] / 10000000)\n",
    "        return df\n",
    "    \n",
    "    def create_lag_df(self, lag_period, df):\n",
    "        lagged_df = df.copy()\n",
    "        # Columns to lag excluding ADD and MALARIA\n",
    "        columns_to_lag = [col for col in df.columns if col not in ['ADD', 'MALARIA']]\n",
    "\n",
    "        # Apply lag for each column and update the original DataFrame\n",
    "        for column in columns_to_lag:\n",
    "            lagged_df[column] = df[column].shift(-lag_period)\n",
    "\n",
    "        # Drop rows with NaN values introduced by the lag\n",
    "        lagged_df.dropna(inplace=True)\n",
    "        lagged_df.reset_index(drop=True, inplace=True)\n",
    "        lagged_df = lagged_df[[\"WMMAX\", \"WMMIN\", \"WTRF\", \"ADD\", \"MALARIA\"]]\n",
    "        scaler = StandardScaler()\n",
    "        standardized_data = scaler.fit_transform(lagged_df)\n",
    "        return lagged_df, standardized_data\n",
    "\n",
    "\n",
    "\n",
    "    def train_som(self,scaled_features, noc, sigma, learning_rate, iterations):\n",
    "        som_shape = (noc, noc)\n",
    "        som = MiniSom(som_shape[0], som_shape[1], scaled_features.shape[1], sigma=sigma, learning_rate=learning_rate,\n",
    "                      neighborhood_function='gaussian', random_seed=10)\n",
    "        som.random_weights_init(scaled_features)\n",
    "\n",
    "        max_iter = iterations\n",
    "        q_error = []\n",
    "        t_error = []\n",
    "\n",
    "        for i in range(max_iter):\n",
    "            rand_i = np.random.randint(len(scaled_features))\n",
    "            som.update(scaled_features[rand_i], som.winner(scaled_features[rand_i]), i, max_iter)\n",
    "            q_error.append(som.quantization_error(scaled_features))\n",
    "            t_error.append(som.topographic_error(scaled_features))\n",
    "\n",
    "        plt.plot(np.arange(len(q_error)), q_error, label='quantization error')\n",
    "        plt.plot(np.arange(len(t_error)), t_error, label='topographic error')\n",
    "        plt.ylabel('Error')\n",
    "        plt.xlabel('Iteration index')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        return som\n",
    "\n",
    "    def get_cluster_info(self,trained_som, scaled_features):\n",
    "        cluster_info = {}\n",
    "        for i, x in enumerate(scaled_features):\n",
    "            winner = trained_som.winner(x)\n",
    "            winner_str = str(winner)\n",
    "            if winner_str in cluster_info:\n",
    "                cluster_info[winner_str]['indices'].append(i)\n",
    "            else:\n",
    "                cluster_info[winner_str] = {'indices': [i]}\n",
    "        return cluster_info\n",
    "\n",
    "    def calculate_statistics(self,cluster_info, df):\n",
    "        cluster_values_df = pd.DataFrame()\n",
    "        for cluster, info in cluster_info.items():\n",
    "            ADD = df.loc[info['indices'], 'ADD'].mean()\n",
    "            MALARIA = df.loc[info['indices'], 'MALARIA'].mean()\n",
    "            WMMAX = df.loc[info['indices'], 'WMMAX'].mean()\n",
    "            WMMIN = df.loc[info['indices'], 'WMMIN'].mean()\n",
    "            WTRF = df.loc[info['indices'], 'WTRF'].mean()\n",
    "            cluster_values_df = cluster_values_df.append({\n",
    "                'ADD': ADD,\n",
    "                'MALARIA': MALARIA,\n",
    "                'WMMAX': WMMAX,\n",
    "                'WMMIN': WMMIN,\n",
    "                'WTRF': WTRF\n",
    "            }, ignore_index=True)\n",
    "\n",
    "        return cluster_values_df\n",
    "\n",
    "    def plot_statistics(self,cluster_values_df):\n",
    "        fig, axs = plt.subplots(3, 2, figsize=(15, 10))\n",
    "        cluster_values_df['ADD'].plot(kind='bar', ax=axs[0, 0], color='skyblue')\n",
    "        axs[0, 0].set_title('Avg ADD')\n",
    "        cluster_values_df['MALARIA'].plot(kind='bar', ax=axs[0, 1], color='black')\n",
    "        axs[0, 1].set_title('Avg MALARIA')\n",
    "        cluster_values_df['WMMAX'].plot(kind='bar', ax=axs[1, 0], color='orange')\n",
    "        axs[1, 0].set_title('Avg WMMAX')\n",
    "        cluster_values_df['WMMIN'].plot(kind='bar', ax=axs[1, 1], color='green')\n",
    "        axs[1, 1].set_title('Avg WMMIN ')\n",
    "        cluster_values_df['WTRF'].plot(kind='bar', ax=axs[2, 0], color='blue')\n",
    "        axs[2, 0].set_title('Avg WTRF')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def analyze_clusters(self,trained_som, scaled_features):\n",
    "        cluster_info = self.get_cluster_info(trained_som, scaled_features)\n",
    "\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        cluster_labels = [f'Cluster {i+1}' for i in range(len(cluster_info))]\n",
    "        plt.subplot(1, 2, 1)\n",
    "        clusters = list(cluster_info.keys())\n",
    "        counts = [len(info['indices']) for info in cluster_info.values()]\n",
    "        bars = plt.bar(range(len(clusters)), counts, color='skyblue')\n",
    "        plt.xticks(range(len(clusters)), cluster_labels, rotation='vertical')\n",
    "        plt.xlabel('Cluster')\n",
    "        plt.ylabel('Number of Data Points')\n",
    "        plt.title('Number of Data Points in Each Cluster')\n",
    "\n",
    "        for bar, count in zip(bars, counts):\n",
    "            y_pos = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width() / 2, y_pos, str(count), ha='center', va='bottom')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def train_and_analyze_soms(self,noc,district_df,sf1, sf2, sf4,train_som, get_cluster_info, calculate_statistics,sigma,learning_rate,iterations):\n",
    "    \n",
    "        scaled_feature_sets = {\n",
    "        'sf1': sf1,\n",
    "        'sf2': sf2,\n",
    "        'sf4': sf4\n",
    "    }\n",
    "        # Dictionary to store the trained SOMs\n",
    "        trained_soms = {}\n",
    "\n",
    "        # Dictionary to store the cluster information\n",
    "        cluster_infos = {}\n",
    "\n",
    "        # Dictionary to store the cluster values dataframes\n",
    "        cluster_values_dfs = {}\n",
    "\n",
    "        # Loop over each scaled feature set\n",
    "        for key, sf in scaled_feature_sets.items():\n",
    "            # Train the SOM for the current scaled feature set\n",
    "\n",
    "            trained_som = train_som(sf, noc,sigma,learning_rate,iterations)\n",
    "            trained_som.train_batch(sf, iterations, verbose=True)\n",
    "\n",
    "            # Store the trained SOM\n",
    "            trained_soms[key] = trained_som\n",
    "\n",
    "            # Get the cluster information for the current trained SOM\n",
    "            cluster_info = get_cluster_info(trained_som, sf)\n",
    "\n",
    "            self.analyze_clusters(trained_som, sf)\n",
    "            # Store the cluster information\n",
    "            self.cluster_infos[key] = cluster_info\n",
    "\n",
    "            # Calculate statistics for the current cluster information\n",
    "            cluster_values_df = calculate_statistics(cluster_info, district_df)\n",
    "\n",
    "            # Store the cluster values dataframe\n",
    "            self.cluster_values_dfs[key] = cluster_values_df\n",
    "\n",
    "\n",
    "            self.plot_statistics(cluster_values_df)\n",
    "        return cluster_values_dfs\n",
    "\n",
    "\n",
    "\n",
    "    def plot_variables(self,df1, df2, df4, noc):\n",
    "        grid_size = math.ceil(math.sqrt(len(df1)))\n",
    "\n",
    "        variables = ['ADD', 'MALARIA', 'WMMAX', 'WMMIN', 'WTRF']  # Replace with actual variable names\n",
    "\n",
    "        # Define the labels for the bars\n",
    "        labels = ['W1', 'W2', 'W4']\n",
    "\n",
    "        # Define the bar width (uniform for all bars)\n",
    "        bar_width = 0.25\n",
    "\n",
    "        color_map = {\n",
    "            'ADD': 'black',\n",
    "            'MALARIA': 'black',\n",
    "            'WMMAX': 'red',\n",
    "            'WMMIN': 'blue',\n",
    "            'WTRF': 'black'\n",
    "        }\n",
    "\n",
    "        # Loop over each variable to create a separate grid of subplots\n",
    "        for variable in variables:\n",
    "            # Create a figure with subplots arranged in a grid\n",
    "            fig, axs = plt.subplots(noc, noc, figsize=(noc+2, noc+2), constrained_layout=True)\n",
    "            axs = axs.flatten()\n",
    "\n",
    "\n",
    "            for i, ax in enumerate(axs):\n",
    "                # For 'ADD' and 'MALARIA', plot only the first value and round to the nearest integer\n",
    "                if variable in ['ADD', 'MALARIA']:\n",
    "                    values = [int(round(df1[variable].iloc[i]))]\n",
    "                    current_labels = ['W1']\n",
    "                else:\n",
    "                    # For other variables, plot all values\n",
    "                    values = [\n",
    "                        df1[variable].iloc[i],\n",
    "                        df2[variable].iloc[i],\n",
    "                        df4[variable].iloc[i]\n",
    "                    ]\n",
    "                    current_labels = labels\n",
    "\n",
    "                # Plot the bars with uniform width\n",
    "                ax.bar(current_labels, values, width=bar_width, color=color_map[variable])\n",
    "\n",
    "                ax.set_ylabel(variable)\n",
    "\n",
    "                # Find the global maximum value for the current variable\n",
    "                max_value = max(df1[variable].max(), df2[variable].max(), df4[variable].max())\n",
    "\n",
    "                if not np.isnan(max_value) and np.isfinite(max_value):  # Set the same y-axis limit for all subplots\n",
    "                    ax.set_ylim(0, max_value + (max_value * 0.1))\n",
    "\n",
    "                # If variable is 'ADD' or 'MALARIA', display integer values\n",
    "                if variable in ['ADD', 'MALARIA']:\n",
    "                    for j, v in enumerate(values):\n",
    "                        # Adjust label position to avoid overlapping bars\n",
    "                        y_pos = v + (max_value * 0.05) if v > 0 else v - (max_value * 0.05)\n",
    "                        ax.text(j, y_pos, str(int(v)), ha='center', va='bottom', fontsize=8)  # Show integer value as string\n",
    "\n",
    "            # Adjust the layout and show the plot for the current variable\n",
    "            plt.suptitle(f'{noc}x{noc} Grid of Subplots for {variable}')\n",
    "            plt.show()\n",
    "\n",
    "    def plots(self, df, noc, iterations, learning_rate, sigma):\n",
    "        # Renamed variable to avoid conflict\n",
    "        w1, sf1 = self.create_lag_df(0, df)\n",
    "        w2, sf2 = self.create_lag_df(2, df)\n",
    "        w4, sf4 = self.create_lag_df(4, df)\n",
    "\n",
    "        # Assuming train_and_analyze_soms and plot_variables are defined elsewhere\n",
    "        cluster_values_dfs = self.train_and_analyze_soms(noc, df, sf1, sf2, sf4, self.train_som,\n",
    "                                                         self.get_cluster_info, self.calculate_statistics,\n",
    "                                                         sigma, learning_rate, iterations)\n",
    "        df1 = self.cluster_values_dfs['sf1']\n",
    "        df2 = self.cluster_values_dfs['sf2']\n",
    "        df4 = self.cluster_values_dfs['sf4']\n",
    "        self.plot_variables(df1, df2, df4, noc)\n",
    "        \n",
    "    def execute_steps(self, hyp):\n",
    "        # Read files\n",
    "        dataframes = self.read_files()\n",
    "\n",
    "        # Population mean normalization\n",
    "        normalized_dfs = []\n",
    "        for df in dataframes:\n",
    "            normalized_df = self.pop_mean(df)\n",
    "            normalized_dfs.append(normalized_df)\n",
    "\n",
    "        # Train and analyze SOMs\n",
    "        for i, df in enumerate(normalized_dfs):\n",
    "            for j in range(5):\n",
    "                self.plots(df, 3, hyp[j][0], hyp[j][1], hyp[j][2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba24b568",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SOMAnalyzer(\"Nagpur.csv\", \"Pune.csv\")\n",
    "\n",
    "# Define hyp (assuming it's defined elsewhere)\n",
    "hyp = [\n",
    "    [5000, 0.2, 3],\n",
    "    [5000, 0.2, 2],\n",
    "    [15000, 0.1, 2],\n",
    "    [25000, 0.05, 2],\n",
    "    [25000, 0.1, 1]\n",
    "]\n",
    "\n",
    "# Execute all steps\n",
    "analyzer.execute_steps(hyp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6c1a4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
